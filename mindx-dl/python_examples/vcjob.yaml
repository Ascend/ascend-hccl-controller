apiVersion: batch.volcano.sh/v1alpha1   # Immutable, must use volcano API
kind: Job                               # Currently, only the Job type is supported.
metadata:
  name: {{ job_name }}                  # Note the correspondence with the name of ConfigMap
  labels:
    ring-controller.atlas: ascend-910   # It is consistent with the label in ConfigMap and cannot be modified
spec:
  minAvailable: {{ node_count }}
  schedulerName: volcano                # Using the volcano scheduler
  policies:
    - event: PodEvicted
      action: RestartJob
  plugins:
    ssh: []
    env: []
    svc: []
  maxRetry: {{ max_retry }}
  queue: default
  tasks:
  - name: "default-test"
    replicas: {{ node_count }}                   # For a single-node system, the value is 1
    template:
      metadata:
        labels:
          app: tf
          ring-controller.atlas: ascend-910      # It is consistent with the label in ConfigMap and cannot be modified
      spec:
        containers:
        - image: {{ docker_image }}              # Docker image for training
          imagePullPolicy: IfNotPresent
          name: tf
          env:
          - name: RANK_TABLE_FILE
            # If you need to modify the data mount path in ConfigMap,
            # you need to modify it to be consistent with the mount path of the following ConfigMap
            value: "/user/serverid/devindex/config/hccl.json"
          - name: {{ job_name }}                               # Be consistent with Jobname
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: XDL_IP                                       # IP of the host, which is used to identify which node the pod runs on
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          command: {{ command }}
          args: {{ args }}
          resources:
            requests:
              huawei.com/Ascend910: {{ npu_count }}
            limits:
              huawei.com/Ascend910: {{ npu_count }}            # At present, it needs to be consistent with the above requests
          volumeMounts:
          - name: ascend-910-config
            mountPath: /user/serverid/devindex/config
          - name: code
            mountPath: {{ container_code_path }}               # Training script path in container
          - name: data
            mountPath: {{ container_data_path }}               # Training dataset path in container
          - name: output
            mountPath: {{ container_output_path }}             # Training output path in container
          - name: localtime
            mountPath: /etc/localtime
        nodeSelector:
          {{ node_selector }}
        volumes:
        - name: ascend-910-config
          configMap:
           name: rings-config-{{ job_name }}   # Name corresponding to ConfigMap
        {% if use_nfs %}                       # Use NFS or not
        - name: code
          nfs:
            server: {{ nfs_server_ip }}        # NFS server address
            path: {{ host_code_path }}         # Training script path in host
        - name: data
          nfs:
            server: {{ nfs_server_ip }}
            path: {{ host_data_path }}         # Training dataset path in host
        - name: output
          nfs:
            server: {{ nfs_server_ip }}
            path: {{ host_output_path }}       # The path to save the model on the host is related to the training script
        {% else %}
        - name: code
          hostPath:
            path: {{ host_code_path }}         # Training script path in host
        - name: data
          hostPath:
            path: {{ host_data_path }}         # Training dataset path in host
        - name: output
          hostPath:
            path: {{ host_output_path }}       # The path to save the model on the host is related to the training script
        {% endif %}
        - name: localtime
          hostPath:
            path: /etc/localtime               # Ensure that the container time is consistent with the host
        restartPolicy: OnFailure