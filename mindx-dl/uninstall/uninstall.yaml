# Copyright Â© Huawei Technologies CO., Ltd. 2020-2020. All rights reserved
---
# This playbook is used to uninstall MindX DL platform.
# Before running the script, ensure that the dependent files are stored in the 'dls_root_dir' directory defined in /etc/ansible/hosts

# This play is used to remove 1).Services 2).Images 3).Logs 4).Log rotate config file on master node.
- hosts: localnode, master
  remote_user: root
  vars_prompt:
    - name: remove_log_files
      prompt: Do you want to remove logs of MindX DL?(yes/no)
      private: no
      default: "no"
    - name: uninstall_k8s_and_docker
      prompt: Do you want to uninstall kubernetes and docker?(yes/no)
      private: no
      default: "no"
    - name: uninstall_nfs
      prompt: Do you want to uninstall NFS?(yes/no)
      private: no
      default: "no"


  tasks:
    # check interactive input
    - name: Check interactive input
      fail:
        msg: "Answer for prompt questions should be 'yes' or 'no', run the script again with valid input."
      when:
        - uninstall_k8s_and_docker not in ['no', 'yes'] or remove_log_files not in ['no', 'yes'] or uninstall_nfs not in ['no', 'yes']

    - name: Record user config
      shell:
        cmd: |
          cat << EOF > "{{playbook_dir}}"/config.yaml
          whether_uninstall_k8s_and_docker: {{uninstall_k8s_and_docker}}
          whether_remove_log_files: {{remove_log_files}}
          whether_uninstall_nfs: {{uninstall_nfs}}
          EOF

    - name: Remove volcano service
      shell:
        cmd:
          kubectl delete CustomResourceDefinition jobs.batch.volcano.sh;
          kubectl delete CustomResourceDefinition commands.bus.volcano.sh;
          kubectl delete CustomResourceDefinition podgroups.scheduling.volcano.sh;
          kubectl delete CustomResourceDefinition queues.scheduling.volcano.sh;
          kubectl delete Job volcano-admission-init -n volcano-system;
          kubectl delete Service volcano-admission-service -n volcano-system;
          kubectl delete Deployment volcano-controllers -n volcano-system;
          kubectl delete Deployment volcano-admission -n volcano-system;
          kubectl delete Deployment volcano-scheduler -n volcano-system;
          kubectl delete ClusterRoleBinding volcano-controllers-role -n volcano-system;
          kubectl delete ClusterRoleBinding volcano-admission-role -n volcano-system;
          kubectl delete ClusterRoleBinding volcano-scheduler-role -n volcano-system;
          kubectl delete ClusterRole volcano-controllers -n volcano-system;
          kubectl delete ClusterRole volcano-admission -n volcano-system;
          kubectl delete ClusterRole volcano-scheduler -n volcano-system;
          kubectl delete ServiceAccount volcano-controllers -n volcano-system;
          kubectl delete ServiceAccount volcano-admission -n volcano-system;
          kubectl delete ServiceAccount volcano-scheduler -n volcano-system;
          kubectl delete ConfigMap volcano-scheduler-configmap -n volcano-system;
          kubectl delete Namespace volcano-system;
      ignore_errors: True

    - name: Remove hccl-controller service
      shell:
        cmd:
          kubectl delete Deployment hccl-controller;
          kubectl delete ClusterRoleBinding hccl-controller-rolebinding;
          kubectl delete ServiceAccount hccl-controller
      ignore_errors: True

    - name: Remove device-plugin service
      shell:
        cmd:
          kubectl delete DaemonSet ascend-device-plugin-daemonset -n kube-system;
          kubectl delete DaemonSet ascend-device-plugin2-daemonset -n kube-system;
          kubectl delete DaemonSet ascend-device-plugin710-daemonset -n kube-system;
          kubectl delete ClusterRoleBinding pods-device-plugin -n kube-system;
      ignore_errors: True

    - name: Remove cadvisor service
      shell:
        cmd:
          kubectl delete DaemonSet cadvisor -n cadvisor;
          kubectl delete ServiceAccount cadvisor -n cadvisor;
          kubectl delete PodSecurityPolicy cadvisor;
          kubectl delete ClusterRoleBinding cadvisor -n cadvisor;
          kubectl delete ClusterRole cadvisor;
          kubectl delete Namespace cadvisor;
      ignore_errors: True

    - name: Check whether all service component services are terminated
      shell:
        cmd:
          kubectl get pods --all-namespaces -o wide
          | grep Terminating >/dev/null 2>&1;
          echo $?
      register: return_value
      until: "'1' in return_value.stdout"
      retries: 6
      delay: 5
      ignore_errors: True

    - name: Remove volcano images
      shell:
        cmd:
          docker rmi -f $(docker images | grep volcanosh | awk '{print $3}')
      ignore_errors: True

    - name: Remove hccl-controller image
      shell:
        cmd:
          docker rmi -f $(docker images | grep hccl-controller | awk '{print $3}')
      ignore_errors: True

    - name: Remove logs
      file:
        path: /var/log/atlas_dls/{{item}}
        state: absent
      with_items:
        - hccl-controller
        - volcano-admission
        - volcano-controller
        - volcano-scheduler
      when: remove_log_files == 'yes'

    - name: Remove log config file
      file:
        path: /etc/logrotate.d/mindx_dl_scheduler
        state: absent



# This play is used to remove 1).Images 2).Logs 3).Log rotate config file on worker nodes.
- hosts: localnode, workers
  remote_user: root
  vars_files:
    - config.yaml

  tasks:
    - name: Remove device-plugin image
      shell:
        docker rmi -f $(docker images | grep ascend-k8sdeviceplugin | awk '{print $3}')
      ignore_errors: True

    - name: Remove cadvisor image
      shell:
        docker rmi -f $(docker images | grep google/cadvisor |awk '{print $3}')
      ignore_errors: True

    - name: Remove logs
      file:
        path: /var/log/{{item}}
        state: absent
      with_items:
        - cadvisor
        - devicePlugin
      when: whether_remove_log_files == true

    - name: Remove log config file
      file:
        path: /etc/logrotate.d/mindx_dl_advisor
        state: absent



# For who choose to remove k8s & docker, this play is used to remove all nodes in cluster.
- hosts: master
  remote_user: root
  vars_files:
    - config.yaml

  tasks:
    - name: Get node names in cluster
      shell:
        cmd: kubectl get nodes | grep -v master | awk '{print $1}'
      register: cluster_node_names
      when: whether_uninstall_k8s_and_docker == true
      ignore_errors: True

    - name: Drain all nodes in kubernetes clusters
      shell:
        cmd:
          kubectl drain {{item}} --delete-local-data --force --ignore-daemonsets
      with_items: "{{cluster_node_names.stdout_lines[1:]}}"
      when: whether_uninstall_k8s_and_docker == true
      ignore_errors: True

    - name: Delete all nodes in kubernetes clusters
      shell:
        cmd:
          kubectl delete node {{item}}
      with_items: "{{cluster_node_names.stdout_lines[1:]}}"
      ignore_errors: True



# This play is used to remove k8s & docker and restore system environment on all nodes(master & cluster).
- hosts: localnode, cluster
  remote_user: root
  vars_files:
    - config.yaml

  tasks:
    - name: Uninstall kubernetes on every node
      shell:
        cmd:
          kubeadm reset -f;
      when: whether_uninstall_k8s_and_docker == true
      ignore_errors: True

    - name: Uninstall kubernetes on every node - Ubuntu
      shell:
        apt-mark unhold kubelet kubeadm kubectl;
        apt-get --purge remove -y kubeadm kubectl kubelet kubernetes-cni kube*;
        apt-get autoremove -y
      ignore_errors: True
      when:
        - whether_uninstall_k8s_and_docker == true
        - ansible_distribution == 'Ubuntu'

    - name: Uninstall kubernetes on every node - CentOS
      shell:
        yum remove -y kubeadm kubectl kubelet kubernetes-cni;
        yum autoremove -y
      ignore_errors: True
      when:
        - whether_uninstall_k8s_and_docker == true
        - ansible_distribution == 'CentOS'

    - name: Remove residule files
      file:
        path: "{{item}}"
        state: absent
      with_items:
        - ~/.kube
        - /etc/kubernetes
        - /etc/systemd/system/kubelet.service.d
        - /etc/systemd/system/kubelet.service
        - /usr/bin/kube*
        - /etc/cni
        - /opt/cni
        - /var/lib/etcd
        - /var/etcd
      when: whether_uninstall_k8s_and_docker == true

    - name: Uninstall docker on every node - Ubuntu
      shell:
        cmd:
          apt-get --purge remove -y docker docker-ce docker-engine docker.io containerd runc;
          apt-get autoremove -y
      ignore_errors: True
      when:
        - whether_uninstall_k8s_and_docker == true
        - ansible_distribution == 'Ubuntu'

    - name: Uninstall docker on every node - CentOS
      shell:
        cmd:
          yum remove -y docker-ce;
          yum autoremove -y
      ignore_errors: True
      when:
        - whether_uninstall_k8s_and_docker == true
        - ansible_distribution == 'CentOS'

    - name: Restore system configs
      shell:
        cmd:
          sed -ri 's/^#*(.*swap.*)/\1/' /etc/fstab;
          swapon /swapfile;
          sed -ri 's/net.ipv4.ip_forward(.*)=(.*)1/net.ipv4.ip_forward = 0/' /etc/sysctl.conf
      when:
        - whether_uninstall_k8s_and_docker == true

    - name: Restore system configs
      file:
        path: "{{item}}"
        state: absent
      with_items:
        - /etc/sysctl.d/k8s.conf
        - /etc/rc.local
      when:
        - whether_uninstall_k8s_and_docker == true


# This play is used to uninstall nfs.
- hosts: localnode, nfs_server
  remote_user: root
  vars_files:
    - config.yaml

  tasks:
    - name: Stop NFS service
      shell:
        cmd:
          systemctl stop nfs-server
      ignore_errors: True
      when:
        - whether_uninstall_nfs == true

    - name: Remove NFS config info
      lineinfile:
        path: /etc/exports
        line: "{{nfs_shared_dir}} *(rw,sync,no_root_squash)"
        state: absent
      when:
        - whether_uninstall_nfs == true

    - name: Uninstall nfs-server - Ubuntu
      shell:
        cmd:
          apt-get --purge remove -y nfs-kernel-server nfs-common rpcbind;
          apt-get autoremove -y
      ignore_errors: True
      when:
        - whether_uninstall_nfs == true
        - ansible_distribution == 'Ubuntu'

    - name: Uninstall nfs-server - CentOS
      shell:
        cmd:
          yum remove -y nfs-utils rpcbind;
          yum autoremove -y
      ignore_errors: True
      when:
        - whether_uninstall_nfs == true
        - ansible_distribution == 'CentOS'

# This play is used to uninstall nfs.
- hosts: cluster
  remote_user: root
  vars_files:
    - config.yaml

  tasks:
    - name: Uninstall nfs-common - Ubuntu
      shell:
        cmd:
          apt-get --purge remove -y nfs-common;
          apt-get autoremove -y
      ignore_errors: True
      when:
        - whether_uninstall_nfs == true
        - ansible_distribution == 'Ubuntu'
        - ansible_default_ipv4['address'] != nfs_service_ip

    - name: Uninstall nfs-common - CentOS
      shell:
        cmd:
          yum remove -y nfs-utils rpcbind;
          yum autoremove -y
      ignore_errors: True
      when:
        - whether_uninstall_nfs == true
        - ansible_distribution == 'CentOS'
        - ansible_default_ipv4['address'] != nfs_service_ip